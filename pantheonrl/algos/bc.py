"""Behavioural Cloning (BC).
Trains policy by applying supervised learning to a fixed dataset of
(observation, action) pairs generated by some expert demonstrator.

https://github.com/HumanCompatibleAI/imitation/blob/
master/src/imitation/algorithms/bc.py
"""

import contextlib
from typing import Any, Callable, Dict, Iterable, Mapping, Optional, Tuple, Type, Union

import gymnasium as gym
import numpy as np
import torch as th
import torch.utils.data as th_data
import tqdm.autonotebook as tqdm
from stable_baselines3.common import policies, utils
from torch.optim.adam import Adam
from torch.optim.optimizer import Optimizer

from pantheonrl.common.trajsaver import TransitionsMinimal, transitions_collate_fn
from pantheonrl.common.util import FeedForward32Policy

log = utils.configure_logger(verbose=0)  # change to 1 for debugging


class BCShell:
    def __init__(self, policy):
        self.policy = policy


def reconstruct_policy(
    policy_path: str,
    device: Union[th.device, str] = "auto",
) -> policies.BasePolicy:
    policy = th.load(policy_path, map_location=utils.get_device(device))  # nosec B614
    assert isinstance(policy, policies.BasePolicy)
    return policy


class ConstantLRSchedule:
    """A callable that returns a constant learning rate."""

    def __init__(self, lr: float = 1e-3):
        """
        Args:
          lr: the constant learning rate that calls to this object will return.
        """
        self.lr = lr

    def __call__(self, _):
        return self.lr


class EpochOrBatchIteratorWithProgress:
    def __init__(
        self,
        data_loader: Iterable[dict],
        n_epochs: Optional[int] = None,
        n_batches: Optional[int] = None,
        on_epoch_end: Optional[Callable[[], None]] = None,
        on_batch_end: Optional[Callable[[], None]] = None,
    ):
        if n_epochs is not None and n_batches is None:
            self.use_epochs = True
        elif n_epochs is None and n_batches is not None:
            self.use_epochs = False
        else:
            raise ValueError(
                "Must provide exactly one of `n_epochs` \
                and `n_batches` arguments."
            )

        self.data_loader = data_loader
        self.n_epochs = n_epochs
        self.n_batches = n_batches
        self.on_epoch_end = on_epoch_end
        self.on_batch_end = on_batch_end

    def __iter__(self) -> Iterable[Tuple[dict, dict]]:  # noqa: C901
        samples_so_far = 0
        epoch_num = 0
        batch_num = 0
        batch_suffix = epoch_suffix = ""
        if self.use_epochs:
            display = tqdm.tqdm(total=self.n_epochs)
            epoch_suffix = f"/{self.n_epochs}"
        else:  # Use batches.
            display = tqdm.tqdm(total=self.n_batches)
            batch_suffix = f"/{self.n_batches}"

        def update_desc():
            display.set_description(
                f"batch: {batch_num}{batch_suffix}  \
                epoch: {epoch_num}{epoch_suffix}"
            )

        with contextlib.closing(display):
            while True:
                update_desc()
                got_data_on_epoch = False
                for batch in self.data_loader:
                    got_data_on_epoch = True
                    batch_num += 1
                    batch_size = len(batch["obs"])
                    assert batch_size > 0
                    samples_so_far += batch_size
                    stats = {
                        "epoch_num": epoch_num,
                        "batch_num": batch_num,
                        "samples_so_far": samples_so_far,
                    }
                    yield batch, stats
                    if self.on_batch_end is not None:
                        self.on_batch_end()
                    if not self.use_epochs:
                        update_desc()
                        display.update(1)

                        assert self.n_batches is not None
                        if batch_num >= self.n_batches:
                            return
                if not got_data_on_epoch:
                    raise AssertionError(
                        f"Data loader returned no data after "
                        f"{batch_num} batches, during epoch "
                        f"{epoch_num} -- did it reset correctly?"
                    )
                epoch_num += 1
                if self.on_epoch_end is not None:
                    self.on_epoch_end()

                if self.use_epochs:
                    update_desc()
                    display.update(1)

                    assert self.n_epochs is not None
                    if epoch_num >= self.n_epochs:
                        return


class BC:
    DEFAULT_BATCH_SIZE: int = 32
    """
    Default batch size for DataLoader automatically constructed from
    Transitions. See `set_expert_data_loader()`.
    """

    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        *,
        policy_class: Type[policies.BasePolicy] = FeedForward32Policy,
        policy_kwargs: Optional[Mapping[str, Any]] = None,
        expert_data: Union[Iterable[Mapping], TransitionsMinimal, None] = None,
        optimizer_cls: Type[Optimizer] = Adam,
        optimizer_kwargs: Optional[Dict[str, Any]] = None,
        ent_weight: float = 1e-3,
        l2_weight: float = 0.0,
        device: Union[str, th.device] = "auto",
    ):
        if optimizer_kwargs:  # noqa: SIM102
            if "weight_decay" in optimizer_kwargs:
                raise ValueError("Use the parameter l2_weight instead of weight_decay.")

        self.action_space = action_space
        self.observation_space = observation_space
        self.policy_class = policy_class
        self.device = device = utils.get_device(device)
        self.policy_kwargs = {
            "observation_space": self.observation_space,
            "action_space": self.action_space,
            "lr_schedule": ConstantLRSchedule(),
        }

        self.policy_kwargs.update(policy_kwargs or {})
        self.device = utils.get_device(device)

        self.policy = self.policy_class(**self.policy_kwargs).to(self.device)  # pytype: disable=not-instantiable
        optimizer_kwargs = optimizer_kwargs or {}

        self.optimizer = optimizer_cls(self.policy.parameters(), **optimizer_kwargs)

        self.expert_data_loader: Optional[Iterable[Mapping]] = None
        self.ent_weight = ent_weight
        self.l2_weight = l2_weight

        if expert_data is not None:
            self.set_expert_data_loader(expert_data)

    def set_expert_data_loader(
        self,
        expert_data: Union[Iterable[Mapping], TransitionsMinimal],
    ) -> None:
        """Set the expert data loader, which yields batches of obs-act pairs.
        Changing the expert data loader on-demand is useful for DAgger and
        other interactive algorithms.
        Args:
             expert_data: Either a Torch `DataLoader`, any other iterator that
                yields dictionaries containing "obs" and "acts" Tensors or
                Numpy arrays, or a `TransitionsMinimal` instance.
                If this is a `TransitionsMinimal` instance, then it is
                automatically converted into a shuffled `DataLoader` with batch
                size `BC.DEFAULT_BATCH_SIZE`.
        """
        if isinstance(expert_data, TransitionsMinimal):
            self.expert_data_loader = th_data.DataLoader(
                expert_data,
                shuffle=True,
                batch_size=BC.DEFAULT_BATCH_SIZE,
                collate_fn=transitions_collate_fn,
            )
        else:
            self.expert_data_loader = expert_data

    def _calculate_loss(
        self,
        obs: Union[th.Tensor, np.ndarray],
        acts: Union[th.Tensor, np.ndarray],
    ) -> Tuple[th.Tensor, Dict[str, float]]:
        """
        Calculate the supervised learning loss used to train the behavioral
        clone.
        Args:
            obs: The observations seen by the expert. If this is a Tensor, then
                gradients are detached first before loss is calculated.
            acts: The actions taken by the expert. If this is a Tensor, then
                its gradients are detached first before loss is calculated.
        Returns:
            loss: The supervised learning loss for the behavioral clone to
                optimize.
            stats_dict: Statistics about the learning process to be logged.
        """
        obs = th.as_tensor(obs, device=self.device).detach()
        acts = th.as_tensor(acts, device=self.device).detach()

        _, log_prob, entropy = self.policy.evaluate_actions(obs, acts)
        prob_true_act = th.exp(log_prob).mean()
        log_prob = log_prob.mean()
        entropy = entropy.mean()

        l2_norms = [th.sum(th.square(w)) for w in self.policy.parameters()]
        # divide by 2 to cancel with gradient of square
        l2_norm = sum(l2_norms) / 2

        ent_loss = -self.ent_weight * entropy
        neglogp = -log_prob
        l2_loss = self.l2_weight * l2_norm
        loss = neglogp + ent_loss + l2_loss

        stats_dict = {
            "neglogp": neglogp.item(),
            "loss": loss.item(),
            "entropy": entropy.item(),
            "ent_loss": ent_loss.item(),
            "prob_true_act": prob_true_act.item(),
            "l2_norm": l2_norm.item(),
            "l2_loss": l2_loss.item(),
        }

        return loss, stats_dict

    def train(
        self,
        *,
        n_epochs: int | None = None,
        n_batches: int | None = None,
        on_epoch_end: Callable[[], None] | None = None,
        on_batch_end: Callable[[], None] | None = None,
        log_interval: int = 100,
    ):
        """Train with supervised learning for some number of epochs.
        Here an 'epoch' is just a complete pass through the expert data loader,
        as set by `self.set_expert_data_loader()`.
        Args:
            n_epochs: Number of complete passes made through expert data before
                ending training. Provide exactly one of `n_epochs` and
                `n_batches`.
            n_batches: Number of batches loaded from dataset before ending
                training. Provide exactly one of `n_epochs` and `n_batches`.
            on_epoch_end: Optional callback with no parameters to run at the
                end of each epoch.
            on_batch_end: Optional callback with no parameters to run at the
                end of each batch.
            log_interval: Log stats after every log_interval batches.
        """
        it = EpochOrBatchIteratorWithProgress(
            self.expert_data_loader,
            n_epochs=n_epochs,
            n_batches=n_batches,
            on_epoch_end=on_epoch_end,
            on_batch_end=on_batch_end,
        )

        batch_num = 0
        """
        for batch, stats_dict_it in it:
            loss, stats_dict_loss = self._calculate_loss(batch["obs"], batch["acts"])

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            if batch_num % log_interval == 0:
                for stats in [stats_dict_it, stats_dict_loss]:
                    for k, v in stats.items():
                        log.record(k, v)
                log.dump(batch_num)
            batch_num += 1
        """
        for batch_num, (batch, stats_dict_it) in enumerate(it):
            loss, stats_dict_loss = self._calculate_loss(batch["obs"], batch["acts"])

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            if batch_num % log_interval == 0:
                for stats in [stats_dict_it, stats_dict_loss]:
                    for k, v in stats.items():
                        log.record(k, v)
                log.dump(batch_num)

    def save_policy(self, policy_path: str) -> None:
        """Save policy to a path. Can be reloaded by `.reconstruct_policy()`.
        Args:
            policy_path: path to save policy to.
        """
        th.save(self.policy, policy_path)  # nosec B614
